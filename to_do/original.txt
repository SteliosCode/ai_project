import pandas as pd
import numpy as np
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import keras
from keras import layers
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import joblib
import seaborn as sns
from sklearn.utils import class_weight

column_names = [
    "age",
    "sex",
    "cp",
    "trestbps",
    "chol",
    "fbs",
    "restecg",
    "thalach",
    "exang",
    "oldpeak",
    "slope",
    "ca",
    "thal",
    "class_attbr"]

#get and clean data
data = pd.read_csv("./data/processed.cleveland.data", header=None, names=column_names, na_values="?")
clean_data = data.dropna() 

#data and clean data showcase
print("Original Data: \n", data)
print("Clean Data: \n", clean_data)


inputs = [
    "age",
    "sex",
    "cp",
    "trestbps",
    "chol",
    "fbs",
    "restecg",
    "thalach",
    "exang",
    "oldpeak",
    "slope",
    "ca",
    "thal"]


#separation x=input and y=target 
X = clean_data[inputs]
#y = clean_data["class_attbr"] low accuracy change to:
y = clean_data["class_attbr"].astype(int)

print("Input data: \n", X.values)
print("Target data: \n", y.values)


#scaling data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("Scaled Input data: \n", X_scaled)

#train/validate/test
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y) #30% temp, 70% train
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp) #15% val, 15% test

print("Training shape: \n", X_train.shape)
print("Validation shape: \n", X_val.shape)
print("Test shape: \n", X_test.shape)



#neural network modeling
model = keras.Sequential([

        layers.InputLayer(input_shape=(X_train.shape[1],), name="input_layer"),

        layers.Dense(128, activation="relu", name="hidden_layer1", kernel_initializer="he_normal", kernel_regularizer=keras.regularizers.l2(0.001)),
        layers.Dense(64, activation="relu", name="hidden_layer2" , kernel_initializer="he_normal", kernel_regularizer=keras.regularizers.l2(0.001)),
        layers.Dense(32, activation="relu", name="hidden_layer3" , kernel_initializer="he_normal", kernel_regularizer=keras.regularizers.l2(0.001)),

        layers.Dense(5, activation="softmax", name="output_layer")
    ])

model.compile(
    optimizer = keras.optimizers.Adam(learning_rate=0.001), #adam adapts learning rate
    loss = "sparse_categorical_crossentropy", # categorical crossentropy (0-4)
    metrics = ["accuracy"]                   #metric for classification
)

print(model.summary())

early = EarlyStopping(monitor="val_loss", patience=20, restore_best_weights=True, verbose=1)
checkpoint = ModelCheckpoint("../models/best_model.keras", monitor="val_loss", save_best_only=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=8, verbose=1)
callbacks = [early, checkpoint, reduce_lr]

class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)

unique_classes = np.unique(y_train)
class_weights = {cls: weight for cls, weight in zip(unique_classes, class_weights)}
print("Class weights:", class_weights)



history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    callbacks=callbacks,
    class_weight=class_weights,
    verbose=1
)


train_loss = history.history["loss"][-1]
train_acc  = history.history["accuracy"][-1]
val_loss   = history.history["val_loss"][-1]
val_acc    = history.history["val_accuracy"][-1]

print(f"Final train loss: {train_loss:.4f}, train acc: {train_acc:.4f}")
print(f"Final val   loss: {val_loss:.4f}, val   acc: {val_acc:.4f}")


# Accuracy
plt.figure(figsize=(10,4))
plt.plot(history.history["accuracy"], label="train_acc")
plt.plot(history.history["val_accuracy"], label="val_acc")
plt.title("Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Loss
plt.figure(figsize=(10,4))
plt.plot(history.history["loss"], label="train_loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.title("Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()


test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f} ({test_acc * 100:.2f}%)")


# ---------------------------------------------
# Predictions
y_probs = model.predict(X_test)             # probabilities for each class
y_pred = np.argmax(y_probs, axis=1)         # choose the class with highest probability

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix (Test set)")
plt.show()

# Classification report
report = classification_report(y_test, y_pred, digits=4)
print("Classification report:\n", report)

# Save the final model and scaler
model.save("../models/final_model.keras", include_optimizer=False)
joblib.dump(scaler, "./scaler/scaler.save")


# --------------------------------------------------
# FINAL SUMMARY (all accuracies in %)
# --------------------------------------------------

# Train & validation accuracy (last epoch)
train_acc_pct = train_acc * 100
val_acc_pct = val_acc * 100

# Test accuracy %
test_acc_pct = test_acc * 100

# Classification report overall accuracy
from sklearn.metrics import accuracy_score
overall_acc_pct = accuracy_score(y_test, y_pred) * 100

print("\n================ FINAL SUMMARY ================")
print(f"Train Accuracy:          {train_acc_pct:.2f}%")
print(f"Validation Accuracy:     {val_acc_pct:.2f}%")
print(f"Test Accuracy:           {test_acc_pct:.2f}%")
print(f"Overall Accuracy (CR):   {overall_acc_pct:.2f}%")
print("================================================\n")
