import os
import random
import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import keras
from keras import layers
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import joblib
import seaborn as sns
from sklearn.utils import class_weight
from keras.models import load_model
import json
from imblearn.over_sampling import SMOTE

#SEED = 42
#random.seed(SEED)
#np.random.seed(SEED)
#tf.random.set_seed(SEED)
#os.environ['TF_DETERMINISTIC_OPS'] = '1'
#os.environ['PYTHONHASHSEED'] = str(SEED)


column_names = ["age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "class_attbr"]

import pandas as pd 

# load and clean data
data = pd.read_csv("./data/processed.cleveland.data", header=None, names=column_names, na_values="?")
clean_data = data.dropna() 

# showcase results
print("Original Data: ", data.shape)
print("Clean Data: ", clean_data.shape)


inputs = ["age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal"]


#separation x=input and y=target 
X = clean_data[inputs]
#y = clean_data["class_attbr"]                     #51% & 64%overal
y = clean_data["class_attbr"].astype(int)          #51% & 64%overal
#y = (clean_data["class_attbr"] > 0).astype(int)   #77% & 95%overal


#train/validate/test split BEFORE scaling
X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_val_raw, X_test_raw, y_val, y_test = train_test_split(X_temp_raw, y_temp, test_size=0.50, random_state=42, stratify=y_temp)


# Apply SMOTE only on training data

sm = SMOTE(random_state=42, k_neighbors=3)
X_train_raw_sm, y_train_sm = sm.fit_resample(X_train_raw, y_train)

print("Before SMOTE:", np.bincount(y_train))
print("After SMOTE:", np.bincount(y_train_sm))

#scaling AFTER split â€” fit only on training data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train_raw_sm)
X_val   = scaler.transform(X_val_raw)
X_test  = scaler.transform(X_test_raw)


print("Training shape: \n", X_train.shape)
print("Validation shape: \n", X_val.shape)
print("Test shape: \n", X_test.shape)



#neural network modeling
model = keras.Sequential([

        layers.InputLayer(input_shape=(X_train.shape[1],), name="input_layer"),

        layers.Dense(22, activation="relu", name="hidden_layer1", kernel_initializer="he_normal",kernel_regularizer=keras.regularizers.l2(0.01)),
        layers.Dropout(0.2), #w/0.5 --> 51,64 w/0.2 --> 68,53
        #layers.BatchNormalization(),
        layers.Dense(19, activation="relu", name="hidden_layer2", kernel_initializer="he_normal"),
        #layers.Dense(4, activation="relu", name="hidden_layer3", kernel_initializer="he_normal"),

        
        layers.Dense(5, activation="softmax", name="output_layer")
    ])

model.compile(
    optimizer = keras.optimizers.Adam(learning_rate = 0.001), #adam adapts learning rate
    loss = "sparse_categorical_crossentropy", # categorical crossentropy (0-4)
    metrics = ["accuracy"]                   # metric for classification
)

print(model.summary())

early = EarlyStopping(monitor="val_loss", patience=20, restore_best_weights=True, verbose=1)

reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=8, verbose=1)
callbacks = [early, reduce_lr]

history = model.fit(
    X_train, y_train_sm,
    validation_data=(X_val, y_val),
    epochs=300,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

train_loss = history.history["loss"][-1]
train_acc  = history.history["accuracy"][-1]
val_loss   = history.history["val_loss"][-1]
val_acc    = history.history["val_accuracy"][-1]

print(f"Final train loss: {train_loss:.4f}, train acc: {train_acc:.4f}")
print(f"Final val   loss: {val_loss:.4f}, val   acc: {val_acc:.4f}")


# Accuracy
plt.figure(figsize=(10,4))
plt.plot(history.history["accuracy"], label="train_acc")
plt.plot(history.history["val_accuracy"], label="val_acc")
plt.title("Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Loss
plt.figure(figsize=(10,4))
plt.plot(history.history["loss"], label="train_loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.title("Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()


test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f} ({test_acc * 100:.2f}%)")


# ---------------------------------------------
# Predictions
y_probs = model.predict(X_test)             # probabilities for each class
y_pred = np.argmax(y_probs, axis=1)         # choose the class with highest probability

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix (Test set)")
plt.show()

# --------------------------------------------------
# FINAL SUMMARY (all accuracies in %)
# --------------------------------------------------

# Train & validation accuracy (last epoch)
train_acc_pct = train_acc * 100
val_acc_pct = val_acc * 100

# Test accuracy %
test_acc_pct = test_acc * 100

# Classification report overall accuracy
from sklearn.metrics import accuracy_score
overall_acc_pct = accuracy_score(y_test, y_pred) * 100

print("\n================ FINAL SUMMARY ================")
print(f"Train Accuracy:          {train_acc_pct:.2f}%")
print(f"Validation Accuracy:     {val_acc_pct:.2f}%")
print(f"Test Accuracy:           {test_acc_pct:.2f}%")
print(f"Overall Accuracy (CR):   {overall_acc_pct:.2f}%")
print("================================================\n")

